{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80985320-d328-4127-8d81-0d61e3850fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "GRID_SEARCH = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eb00f8-5fdc-436b-ba1a-b8e9507bb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import pytorchcv\n",
    "import torchvision\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f9dba8-e4da-4307-a08b-033e0e78e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8') # pretty matplotlib plots\n",
    "sns.set('notebook', style='whitegrid', font_scale=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f401bb79-f610-48e7-9d90-e571666ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdab05b-5e8c-4e12-8bb8-90ff06a8de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joseph280996/Code/School/L3D/Project/l3d_pn_dataset1000\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda' # TODO change to GPU if you have one (e.g. on Colab)\n",
    "device = 'cuda'\n",
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda'\n",
    "# else:\n",
    "#     device = 'cpu'\n",
    "\n",
    "host = 'hpc'\n",
    "\n",
    "if host == 'hpc':\n",
    "    DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath('../l3d_pn_dataset1000'))\n",
    "else:\n",
    "    if os.name == 'nt':\n",
    "        # DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\l3d_pn_dataset1000\"))\n",
    "        DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\l3d_pn_dataset500\"))\n",
    "        # DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\Quotient-train\\\\\"))\n",
    "    else:\n",
    "        DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath('./l3d_pn_dataset1000'))\n",
    "\n",
    "print(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1cc335-870f-4190-ba1e-20f5f1da9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFalse_PN\u001b[m\u001b[m \u001b[34mTrue_PN\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d04661e-51c0-42d7-8d02-49df2446acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af076891-a766-46b9-9a80-520148d8950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from provided local starter code files\n",
    "import data_utils\n",
    "import data_utils_pseudo\n",
    "import data_utils_pseudo_2\n",
    "import models\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab1cf85-1e4b-4a81-83e1-12f495720158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy of a model on the test loader.\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        device: 'cuda' or 'cpu'\n",
    "        test_loader: DataLoader for test data\n",
    "\n",
    "    Returns:\n",
    "        Accuracy as a float\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            correct += (predicted == y).sum().item()  # Count correct predictions\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bafa3ea-683d-4390-9ef6-d833ad42ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(best_info):\n",
    "    \"\"\"\n",
    "    Plot training and validation progress.\n",
    "    Args:\n",
    "        best_info: Dictionary containing 'epochs', 'tr' (training), and 'va' (validation) metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Training metrics\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Training Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Training Error')\n",
    "\n",
    "    # Validation metrics\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.title('Training and Validation Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33822b8e-f81b-4eb8-9035-352689a6291e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       splitname   0   1\n",
      "   train_labeled 223 177\n",
      "train_unlabeled1 225 175\n",
      "train_unlabeled2 225 175\n",
      "           valid  25  25\n",
      "            test  25  25\n",
      "Epoch 1/50\n",
      "  Train Loss: 3.6410, Train Error: 0.4475\n",
      "  Val Loss: 0.7258, Val Error: 0.5400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 127\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Track training metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs152_env/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs152_env/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs152_env/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not GRID_SEARCH:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # MixMatch hyperparameters\n",
    "    mixup_alpha = 0.75\n",
    "    temperature = 0.5\n",
    "    lambda_u = 10\n",
    "    num_classes = 2  # Adjust to your dataset\n",
    "    \n",
    "    # Define sharpening function\n",
    "    def sharpen(probabilities, T):\n",
    "        return torch.pow(probabilities, 1 / T) / torch.sum(torch.pow(probabilities, 1 / T), dim=1, keepdim=True)\n",
    "    \n",
    "    # Define mixup function\n",
    "    def mixup(x1, y1, x2, y2, alpha):\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "        lam = max(lam, 1 - lam)\n",
    "        x_mix = lam * x1 + (1 - lam) * x2\n",
    "        y_mix = lam * y1 + (1 - lam) * y2\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "    class ResNetFeatureExtractor(nn.Module):\n",
    "        def __init__(self, num_classes, dropout_rate=0.3):\n",
    "            super(ResNetFeatureExtractor, self).__init__()\n",
    "            # Use a deeper ResNet architecture\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            \n",
    "            # Remove final layers\n",
    "            self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            \n",
    "            # Add custom classification head\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            \n",
    "            # Two fully connected layers with batch norm\n",
    "            self.fc1 = nn.Linear(2048, 512)\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            features = self.feature_extractor(x)\n",
    "            features = self.avgpool(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            features = self.dropout(features)\n",
    "            features = F.relu(self.bn1(self.fc1(features)))\n",
    "            features = self.dropout(features)\n",
    "            out = self.fc2(features)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    root_path = DATA_DIR\n",
    "    tr_loader, unloader1, unloader2, va_loader, test_loader  = data_utils_pseudo.make_PN_data_loaders_with_unlabeled(\n",
    "        root=root_path,\n",
    "        batch_size=32,\n",
    "        frac_valid=50/850\n",
    "    )\n",
    "    \n",
    "    # Model, optimizer, and criterion\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Data collection for training visualization\n",
    "    best_info = {\n",
    "        'epochs': [],\n",
    "        'tr': {'loss': [], 'err': []},\n",
    "        'va': {'xent': [], 'err': []},\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_error = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    results = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "    \n",
    "        for (inputs_x, targets_x), inputs_u1, inputs_u2 in zip(tr_loader, unloader1, unloader2):\n",
    "            # Transfer to device\n",
    "            inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "            inputs_u1 = inputs_u1[0].to(device)  # Unpack inputs from unloader's batch\n",
    "            inputs_u2 = inputs_u2[0].to(device)  # Unpack inputs from unloader's batch\n",
    "    \n",
    "            # Forward pass for unlabeled data\n",
    "            with torch.no_grad():\n",
    "                outputs_u1 = model(inputs_u1)\n",
    "                outputs_u2 = model(inputs_u2)\n",
    "                pseudo_labels1 = torch.softmax(outputs_u1, dim=1)\n",
    "                pseudo_labels2 = torch.softmax(outputs_u2, dim=1)\n",
    "                p = (pseudo_labels1 + pseudo_labels2) / 2\n",
    "                pseudo_labels= sharpen(p, temperature)\n",
    "    \n",
    "            # Combine labeled and pseudo-labeled data\n",
    "            all_inputs = torch.cat([inputs_x, inputs_u1, inputs_u2], dim=0)\n",
    "            all_labels = torch.cat([\n",
    "                torch.nn.functional.one_hot(targets_x, num_classes).float(),\n",
    "                pseudo_labels,\n",
    "                pseudo_labels,\n",
    "            ], dim=0)\n",
    "            inputs_mixed, labels_mixed = mixup(all_inputs, all_labels, all_inputs, all_labels, mixup_alpha)\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs_mixed)\n",
    "            supervised_loss = criterion(outputs[:len(targets_x)], targets_x)\n",
    "            # unsupervised_loss = -(labels_mixed[len(targets_x):] * torch.log_softmax(outputs[len(targets_x):], dim=1)).sum(dim=1).mean()\n",
    "            unsupervised_loss = F.mse_loss(outputs[len(targets_x):], labels_mixed[len(targets_x):])\n",
    "            loss = supervised_loss + lambda_u * unsupervised_loss\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Track training metrics\n",
    "            total_loss += loss.item() * inputs_x.size(0)\n",
    "            _, predicted = outputs[:len(targets_x)].max(1)\n",
    "            correct += predicted.eq(targets_x).sum().item()\n",
    "            total_samples += inputs_x.size(0)\n",
    "    \n",
    "        # Training metrics\n",
    "        train_loss = total_loss / total_samples\n",
    "        train_err = 1 - correct / total_samples\n",
    "        best_info['tr']['loss'].append(train_loss)\n",
    "        best_info['tr']['err'].append(train_err)\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs_val, targets_val in va_loader:\n",
    "                inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "                outputs_val = model(inputs_val)\n",
    "                loss_val = criterion(outputs_val, targets_val)\n",
    "    \n",
    "                val_loss += loss_val.item() * inputs_val.size(0)\n",
    "                _, predicted_val = outputs_val.max(1)\n",
    "                val_correct += predicted_val.eq(targets_val).sum().item()\n",
    "                val_samples += inputs_val.size(0)\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Validation metrics\n",
    "        val_xent = val_loss / val_samples\n",
    "        val_err = 1 - val_correct / val_samples\n",
    "        val_acc = val_correct / val_samples\n",
    "        best_info['va']['xent'].append(val_xent)\n",
    "        best_info['va']['err'].append(val_err)\n",
    "    \n",
    "        # Save results for this epoch\n",
    "        results.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_err\": val_err,\n",
    "            \"config\": {\"learning_rate\": optimizer.param_groups[0]['lr'], \"mixup_alpha\": mixup_alpha, \"lambda_u\": lambda_u},\n",
    "            \"model\": model.state_dict()  # Save model state dict\n",
    "        })\n",
    "        \n",
    "        \n",
    "        # Track epochs\n",
    "        best_info['epochs'].append(epoch)\n",
    "    \n",
    "            # Early stopping logic\n",
    "        if val_err < best_val_error:\n",
    "            best_val_error = val_err\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Error: {train_err:.4f}\")\n",
    "        print(f\"  Val Loss: {val_xent:.4f}, Val Error: {val_err:.4f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Train Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Train Error')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Xent')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa708489-d08f-475b-93c9-8cab771727c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    # Sort results by validation accuracy\n",
    "    results = sorted(results, key=lambda x: x[\"val_acc\"], reverse=True)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nTop 5 Results:\")\n",
    "    for res in results[:5]:\n",
    "        print(f\"Epoch: {res['epoch']}, Config: {res['config']}, Val Accuracy: {res['val_acc']:.4f}\")\n",
    "    \n",
    "    # Retrieve the best configuration and model\n",
    "    best_result = results[0]\n",
    "    best_model_state = best_result[\"model\"]\n",
    "    \n",
    "    # Load the best model if needed\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50606306-2d0c-4ce3-a6ae-afa6c3b608b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    # plt.figure(figsize=(4, 4))  # Set the dimensions of the figure to 4x4 inches\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Train Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Train Error')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Xent')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "    plt.title('MixMatch SSL method')\n",
    "    # Add axis labels\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Add legend and display the plot\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b0df0-06cc-45fb-9968-3e3462210e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    tar_acc = {}\n",
    "    tar_acc[('ResNet50', 'ImageNet1k')] = eval_acc(model, device, test_loader)\n",
    "    print(tar_acc)\n",
    "\n",
    "    test_accuracy = eval_acc(model, device, test_loader)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a962d-5871-4122-9cd7-31f37fdd9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SEARCH:\n",
    "    import itertools\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # MixMatch hyperparameters\n",
    "    mixup_alpha = 0.75\n",
    "    temperature = 0.5\n",
    "    lambda_u = 10\n",
    "    num_classes = 2  # Adjust to your dataset\n",
    "\n",
    "    # Data collection for training visualization\n",
    "    best_info = {\n",
    "        'epochs': [],\n",
    "        'tr': {'loss': [], 'err': []},\n",
    "        'va': {'xent': [], 'err': []},\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_error = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    results = []\n",
    "    \n",
    "    # Define sharpening function\n",
    "    def sharpen(probabilities, T):\n",
    "        return torch.pow(probabilities, 1 / T) / torch.sum(torch.pow(probabilities, 1 / T), dim=1, keepdim=True)\n",
    "    \n",
    "    # Define mixup function\n",
    "    def mixup(x1, y1, x2, y2, alpha):\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "        lam = max(lam, 1 - lam)\n",
    "        x_mix = lam * x1 + (1 - lam) * x2\n",
    "        y_mix = lam * y1 + (1 - lam) * y2\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "    class ResNetFeatureExtractor(nn.Module):\n",
    "        def __init__(self, num_classes, dropout_rate=0.3):\n",
    "            super(ResNetFeatureExtractor, self).__init__()\n",
    "            # Use a deeper ResNet architecture\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            \n",
    "            # Remove final layers\n",
    "            self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            \n",
    "            # Add custom classification head\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            \n",
    "            # Two fully connected layers with batch norm\n",
    "            self.fc1 = nn.Linear(2048, 512)\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            features = self.feature_extractor(x)\n",
    "            features = self.avgpool(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            features = self.dropout(features)\n",
    "            features = F.relu(self.bn1(self.fc1(features)))\n",
    "            features = self.dropout(features)\n",
    "            out = self.fc2(features)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    root_path = DATA_DIR\n",
    "    tr_loader, unloader, va_loader, test_loader  = data_utils_pseudo.make_PN_data_loaders_with_unlabeled(\n",
    "        root=root_path,\n",
    "        batch_size=32,\n",
    "        n_samples_per_class_trainandvalid=500\n",
    "    )\n",
    "    \n",
    "    # Model, optimizer, and criterion\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Define the grid search function\n",
    "    def train_and_evaluate(lr, lambda_u, temperature, mixup_alpha, num_epochs=50):\n",
    "        # Model, optimizer, and criterion\n",
    "        model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # Training loop\n",
    "        best_val_error = float('inf')\n",
    "        patience_counter = 0\n",
    "        results = []\n",
    "    \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total_samples = 0\n",
    "    \n",
    "            for (inputs_x, targets_x), inputs_u in zip(tr_loader, unloader):\n",
    "                # Transfer to device\n",
    "                inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "                inputs_u = inputs_u[0].to(device)\n",
    "    \n",
    "                # Forward pass for unlabeled data\n",
    "                with torch.no_grad():\n",
    "                    outputs_u = model(inputs_u)\n",
    "                    pseudo_labels = torch.softmax(outputs_u, dim=1)\n",
    "                    pseudo_labels = sharpen(pseudo_labels, temperature)\n",
    "    \n",
    "                # Combine labeled and pseudo-labeled data\n",
    "                all_inputs = torch.cat([inputs_x, inputs_u], dim=0)\n",
    "                all_labels = torch.cat([\n",
    "                    torch.nn.functional.one_hot(targets_x, num_classes).float(),\n",
    "                    pseudo_labels\n",
    "                ], dim=0)\n",
    "                inputs_mixed, labels_mixed = mixup(all_inputs, all_labels, all_inputs, all_labels, mixup_alpha)\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(inputs_mixed)\n",
    "\n",
    "                supervised_loss = criterion(outputs[:len(targets_x)], targets_x)\n",
    "    \n",
    "                # Compute unsupervised loss (set reduction='sum' and normalize manually for per-sample loss)\n",
    "                unsupervised_loss = F.mse_loss(\n",
    "                    outputs[len(targets_x):], labels_mixed[len(targets_x):])\n",
    "                \n",
    "                # Combine supervised and unsupervised losses\n",
    "                loss = supervised_loss + lambda_u * unsupervised_loss\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track training metrics\n",
    "                total_loss += loss.item() * len(targets_x)  # Accumulate total loss, scaled by batch size\n",
    "                total_samples += inputs_x.size(0) + inputs_u.size(0)  # Track total number of samples\n",
    "                _, predicted = outputs[:len(targets_x)].max(1)\n",
    "                correct += predicted.eq(targets_x).sum().item()\n",
    "        \n",
    "            # Training metrics\n",
    "            train_loss = total_loss / total_samples\n",
    "            train_err = 1 - correct / total_samples\n",
    "            best_info['tr']['loss'].append(train_loss)\n",
    "            best_info['tr']['err'].append(train_err)\n",
    "        \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs_val, targets_val in va_loader:\n",
    "                    inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "            \n",
    "                    # Forward pass\n",
    "                    outputs_val = model(inputs_val)\n",
    "            \n",
    "                    # Compute validation loss (per-sample average)\n",
    "                    loss_val = criterion(outputs_val, targets_val)  # CrossEntropyLoss defaults to mean\n",
    "                    val_loss += loss_val.item() * inputs_val.size(0)  # Scale by batch size to get total loss for this batch\n",
    "            \n",
    "                    # Compute accuracy\n",
    "                    _, predicted_val = outputs_val.max(1)\n",
    "                    val_correct += predicted_val.eq(targets_val).sum().item()\n",
    "                    val_samples += inputs_val.size(0)\n",
    "            \n",
    "            # Compute per-sample average loss across the entire validation set\n",
    "            val_xent = val_loss / val_samples\n",
    "            val_err = 1 - val_correct / val_samples\n",
    "            val_acc = val_correct / val_samples\n",
    "    \n",
    "            # Early stopping logic\n",
    "            if val_err < best_val_error:\n",
    "                best_val_error = val_err\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "            # Logging\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {total_loss / total_samples:.4f}, Train Error: {1 - correct / total_samples:.4f}\")\n",
    "            print(f\"  Val Loss: {val_xent:.4f}, Val Error: {val_err:.4f}\")\n",
    "    \n",
    "        return val_err, model.state_dict()\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"lr\": [0.0001, 0.00001],\n",
    "        \"lambda_u\": [10, 30, 50, 80],\n",
    "        \"temperature\": [0.5, 1.0],\n",
    "        \"mixup_alpha\": [0.5, 0.75, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    best_model_state = None\n",
    "    best_hyperparams = None\n",
    "    best_val_error = float('inf')\n",
    "    \n",
    "    for lr, lambda_u, temperature, mixup_alpha in itertools.product(\n",
    "            param_grid[\"lr\"], param_grid[\"lambda_u\"], param_grid[\"temperature\"], param_grid[\"mixup_alpha\"]):\n",
    "        print(f\"Testing configuration: lr={lr}, lambda_u={lambda_u}, temperature={temperature}, mixup_alpha={mixup_alpha}\")\n",
    "        val_err, model_state = train_and_evaluate(lr, lambda_u, temperature, mixup_alpha)\n",
    "    \n",
    "        if val_err < best_val_error:\n",
    "            best_val_error = val_err\n",
    "            best_model_state = model_state\n",
    "            best_hyperparams = {\"lr\": lr, \"lambda_u\": lambda_u, \"temperature\": temperature, \"mixup_alpha\": mixup_alpha}\n",
    "    \n",
    "    # Save the best model and hyperparameters\n",
    "    torch.save(best_model_state, \"best_model_grid_search.pth\")\n",
    "    print(f\"Best validation error: {best_val_error}\")\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88051dc4-ad94-401c-98e1-dc6676160c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20242b1c-93ae-46f1-8440-5f4d83e7cf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs152_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
