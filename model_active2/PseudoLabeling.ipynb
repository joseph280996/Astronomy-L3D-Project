{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80985320-d328-4127-8d81-0d61e3850fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "GRID_SEARCH = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eb00f8-5fdc-436b-ba1a-b8e9507bb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import pytorchcv\n",
    "import torchvision\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f9dba8-e4da-4307-a08b-033e0e78e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8') # pretty matplotlib plots\n",
    "sns.set('notebook', style='whitegrid', font_scale=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f401bb79-f610-48e7-9d90-e571666ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdab05b-5e8c-4e12-8bb8-90ff06a8de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\Code\\School\\L3D\\Project\\l3d_pn_dataset1000\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda' # TODO change to GPU if you have one (e.g. on Colab)\n",
    "device = 'cuda'\n",
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda'\n",
    "# else:\n",
    "#     device = 'cpu'\n",
    "\n",
    "host = 'hpc'\n",
    "\n",
    "if host == 'hpc':\n",
    "    DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath('../l3d_pn_dataset1000'))\n",
    "else:\n",
    "    if os.name == 'nt':\n",
    "        # DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\l3d_pn_dataset1000\"))\n",
    "        DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\l3d_pn_dataset500\"))\n",
    "        # DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"C:\\\\Users\\\\arman\\\\Downloads\\\\L3D_Project\\\\Quotient-train\\\\\"))\n",
    "    else:\n",
    "        DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath('./l3d_pn_dataset1000'))\n",
    "\n",
    "print(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1cc335-870f-4190-ba1e-20f5f1da9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False_PN\n",
      "True_PN\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d04661e-51c0-42d7-8d02-49df2446acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af076891-a766-46b9-9a80-520148d8950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from provided local starter code files\n",
    "import data_utils\n",
    "import data_utils_pseudo\n",
    "import data_utils_pseudo_2\n",
    "import models\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab1cf85-1e4b-4a81-83e1-12f495720158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy of a model on the test loader.\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        device: 'cuda' or 'cpu'\n",
    "        test_loader: DataLoader for test data\n",
    "\n",
    "    Returns:\n",
    "        Accuracy as a float\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            correct += (predicted == y).sum().item()  # Count correct predictions\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bafa3ea-683d-4390-9ef6-d833ad42ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(best_info):\n",
    "    \"\"\"\n",
    "    Plot training and validation progress.\n",
    "    Args:\n",
    "        best_info: Dictionary containing 'epochs', 'tr' (training), and 'va' (validation) metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Training metrics\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Training Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Training Error')\n",
    "\n",
    "    # Validation metrics\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.title('Training and Validation Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33822b8e-f81b-4eb8-9035-352689a6291e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # MixMatch hyperparameters\n",
    "    mixup_alpha = 0.75\n",
    "    temperature = 0.5\n",
    "    lambda_u = 10\n",
    "    num_classes = 2  # Adjust to your dataset\n",
    "    \n",
    "    # Define sharpening function\n",
    "    def sharpen(probabilities, T):\n",
    "        return torch.pow(probabilities, 1 / T) / torch.sum(torch.pow(probabilities, 1 / T), dim=1, keepdim=True)\n",
    "    \n",
    "    # Define mixup function\n",
    "    def mixup(x1, y1, x2, y2, alpha):\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "        lam = max(lam, 1 - lam)\n",
    "        x_mix = lam * x1 + (1 - lam) * x2\n",
    "        y_mix = lam * y1 + (1 - lam) * y2\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "    class ResNetFeatureExtractor(nn.Module):\n",
    "        def __init__(self, num_classes, dropout_rate=0.3):\n",
    "            super(ResNetFeatureExtractor, self).__init__()\n",
    "            # Use a deeper ResNet architecture\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            \n",
    "            # Remove final layers\n",
    "            self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            \n",
    "            # Add custom classification head\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            \n",
    "            # Two fully connected layers with batch norm\n",
    "            self.fc1 = nn.Linear(2048, 512)\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            features = self.feature_extractor(x)\n",
    "            features = self.avgpool(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            features = self.dropout(features)\n",
    "            features = F.relu(self.bn1(self.fc1(features)))\n",
    "            features = self.dropout(features)\n",
    "            out = self.fc2(features)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    root_path = DATA_DIR\n",
    "    tr_loader, unloader1, unloader2, va_loader, test_loader  = data_utils_pseudo.make_PN_data_loaders_with_unlabeled(\n",
    "        root=root_path,\n",
    "        batch_size=32,\n",
    "        frac_valid=50/850\n",
    "    )\n",
    "    \n",
    "    # Model, optimizer, and criterion\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Data collection for training visualization\n",
    "    best_info = {\n",
    "        'epochs': [],\n",
    "        'tr': {'loss': [], 'err': []},\n",
    "        'va': {'xent': [], 'err': []},\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_error = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    results = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "    \n",
    "        for (inputs_x, targets_x), inputs_u1, inputs_u2 in zip(tr_loader, unloader1, unloader2):\n",
    "            # Transfer to device\n",
    "            inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "            inputs_u1 = inputs_u1[0].to(device)  # Unpack inputs from unloader's batch\n",
    "            inputs_u2 = inputs_u2[0].to(device)  # Unpack inputs from unloader's batch\n",
    "    \n",
    "            # Forward pass for unlabeled data\n",
    "            with torch.no_grad():\n",
    "                outputs_u1 = model(inputs_u1)\n",
    "                outputs_u2 = model(inputs_u2)\n",
    "                pseudo_labels1 = torch.softmax(outputs_u1, dim=1)\n",
    "                pseudo_labels2 = torch.softmax(outputs_u2, dim=1)\n",
    "                p = (pseudo_labels1 + pseudo_labels2) / 2\n",
    "                pseudo_labels= sharpen(p, temperature)\n",
    "    \n",
    "            # Combine labeled and pseudo-labeled data\n",
    "            all_inputs = torch.cat([inputs_x, inputs_u1, inputs_u2], dim=0)\n",
    "            all_labels = torch.cat([\n",
    "                torch.nn.functional.one_hot(targets_x, num_classes).float(),\n",
    "                pseudo_labels,\n",
    "                pseudo_labels,\n",
    "            ], dim=0)\n",
    "            inputs_mixed, labels_mixed = mixup(all_inputs, all_labels, all_inputs, all_labels, mixup_alpha)\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs_mixed)\n",
    "            supervised_loss = criterion(outputs[:len(targets_x)], targets_x)\n",
    "            # unsupervised_loss = -(labels_mixed[len(targets_x):] * torch.log_softmax(outputs[len(targets_x):], dim=1)).sum(dim=1).mean()\n",
    "            unsupervised_loss = F.mse_loss(outputs[len(targets_x):], labels_mixed[len(targets_x):])\n",
    "            loss = supervised_loss + lambda_u * unsupervised_loss\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Track training metrics\n",
    "            total_loss += loss.item() * inputs_x.size(0)\n",
    "            _, predicted = outputs[:len(targets_x)].max(1)\n",
    "            correct += predicted.eq(targets_x).sum().item()\n",
    "            total_samples += inputs_x.size(0)\n",
    "    \n",
    "        # Training metrics\n",
    "        train_loss = total_loss / total_samples\n",
    "        train_err = 1 - correct / total_samples\n",
    "        best_info['tr']['loss'].append(train_loss)\n",
    "        best_info['tr']['err'].append(train_err)\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs_val, targets_val in va_loader:\n",
    "                inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "                outputs_val = model(inputs_val)\n",
    "                loss_val = criterion(outputs_val, targets_val)\n",
    "    \n",
    "                val_loss += loss_val.item() * inputs_val.size(0)\n",
    "                _, predicted_val = outputs_val.max(1)\n",
    "                val_correct += predicted_val.eq(targets_val).sum().item()\n",
    "                val_samples += inputs_val.size(0)\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Validation metrics\n",
    "        val_xent = val_loss / val_samples\n",
    "        val_err = 1 - val_correct / val_samples\n",
    "        val_acc = val_correct / val_samples\n",
    "        best_info['va']['xent'].append(val_xent)\n",
    "        best_info['va']['err'].append(val_err)\n",
    "    \n",
    "        # Save results for this epoch\n",
    "        results.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_err\": val_err,\n",
    "            \"config\": {\"learning_rate\": optimizer.param_groups[0]['lr'], \"mixup_alpha\": mixup_alpha, \"lambda_u\": lambda_u},\n",
    "            \"model\": model.state_dict()  # Save model state dict\n",
    "        })\n",
    "        \n",
    "        \n",
    "        # Track epochs\n",
    "        best_info['epochs'].append(epoch)\n",
    "    \n",
    "            # Early stopping logic\n",
    "        if val_err < best_val_error:\n",
    "            best_val_error = val_err\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Error: {train_err:.4f}\")\n",
    "        print(f\"  Val Loss: {val_xent:.4f}, Val Error: {val_err:.4f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Train Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Train Error')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Xent')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa708489-d08f-475b-93c9-8cab771727c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    # Sort results by validation accuracy\n",
    "    results = sorted(results, key=lambda x: x[\"val_acc\"], reverse=True)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nTop 5 Results:\")\n",
    "    for res in results[:5]:\n",
    "        print(f\"Epoch: {res['epoch']}, Config: {res['config']}, Val Accuracy: {res['val_acc']:.4f}\")\n",
    "    \n",
    "    # Retrieve the best configuration and model\n",
    "    best_result = results[0]\n",
    "    best_model_state = best_result[\"model\"]\n",
    "    \n",
    "    # Load the best model if needed\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50606306-2d0c-4ce3-a6ae-afa6c3b608b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    # plt.figure(figsize=(4, 4))  # Set the dimensions of the figure to 4x4 inches\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['loss'], '--', color='b', label='Train Loss')\n",
    "    plt.plot(best_info['epochs'], best_info['tr']['err'], '-', color='b', label='Train Error')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['xent'], '--', color='r', label='Validation Xent')\n",
    "    plt.plot(best_info['epochs'], best_info['va']['err'], '-', color='r', label='Validation Error')\n",
    "    plt.title('MixMatch SSL method')\n",
    "    # Add axis labels\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Add legend and display the plot\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98b0df0-06cc-45fb-9968-3e3462210e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GRID_SEARCH:\n",
    "    tar_acc = {}\n",
    "    tar_acc[('ResNet50', 'ImageNet1k')] = eval_acc(model, device, test_loader)\n",
    "    print(tar_acc)\n",
    "\n",
    "    test_accuracy = eval_acc(model, device, test_loader)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3a962d-5871-4122-9cd7-31f37fdd9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       splitname   0   1\n",
      "   train_labeled 206 194\n",
      "train_unlabeled1 218 182\n",
      "train_unlabeled2 218 182\n",
      "           valid  25  25\n",
      "            test  50  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\josep/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:08<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=0.5, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.2430, Train Error: 0.8367\n",
      "  Val Loss: 0.6996, Val Error: 0.5000\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8913, Train Error: 0.8167\n",
      "  Val Loss: 0.6973, Val Error: 0.4600\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.7313, Train Error: 0.8067\n",
      "  Val Loss: 0.6979, Val Error: 0.5200\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.6684, Train Error: 0.8208\n",
      "  Val Loss: 0.6828, Val Error: 0.5200\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5906, Train Error: 0.8117\n",
      "  Val Loss: 0.6945, Val Error: 0.5600\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5715, Train Error: 0.8033\n",
      "  Val Loss: 0.6822, Val Error: 0.4800\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4854, Train Error: 0.7900\n",
      "  Val Loss: 0.6846, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4762, Train Error: 0.8058\n",
      "  Val Loss: 0.6847, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4614, Train Error: 0.7842\n",
      "  Val Loss: 0.6891, Val Error: 0.5200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.4250, Train Error: 0.8000\n",
      "  Val Loss: 0.6801, Val Error: 0.4800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.4068, Train Error: 0.7842\n",
      "  Val Loss: 0.6886, Val Error: 0.5000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3850, Train Error: 0.8033\n",
      "  Val Loss: 0.6882, Val Error: 0.4000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3634, Train Error: 0.7817\n",
      "  Val Loss: 0.6861, Val Error: 0.4600\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3524, Train Error: 0.7808\n",
      "  Val Loss: 0.6834, Val Error: 0.4600\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.3388, Train Error: 0.7767\n",
      "  Val Loss: 0.6901, Val Error: 0.4600\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.3332, Train Error: 0.7725\n",
      "  Val Loss: 0.6841, Val Error: 0.4600\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.3205, Train Error: 0.7717\n",
      "  Val Loss: 0.6881, Val Error: 0.4800\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.3154, Train Error: 0.7617\n",
      "  Val Loss: 0.6810, Val Error: 0.5000\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.3101, Train Error: 0.7667\n",
      "  Val Loss: 0.6854, Val Error: 0.5000\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.3112, Train Error: 0.7717\n",
      "  Val Loss: 0.6854, Val Error: 0.4600\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2928, Train Error: 0.7758\n",
      "  Val Loss: 0.6846, Val Error: 0.4600\n",
      "Early stopping triggered after epoch 22\n",
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=0.5, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.1295, Train Error: 0.8342\n",
      "  Val Loss: 0.6772, Val Error: 0.4000\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8024, Train Error: 0.8267\n",
      "  Val Loss: 0.6675, Val Error: 0.4000\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.7157, Train Error: 0.8058\n",
      "  Val Loss: 0.6672, Val Error: 0.4000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.6124, Train Error: 0.7958\n",
      "  Val Loss: 0.6686, Val Error: 0.4200\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5569, Train Error: 0.7950\n",
      "  Val Loss: 0.6649, Val Error: 0.3800\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5139, Train Error: 0.7883\n",
      "  Val Loss: 0.6660, Val Error: 0.2800\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4777, Train Error: 0.7908\n",
      "  Val Loss: 0.6561, Val Error: 0.3000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4738, Train Error: 0.7992\n",
      "  Val Loss: 0.6512, Val Error: 0.3000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4186, Train Error: 0.7942\n",
      "  Val Loss: 0.6627, Val Error: 0.3000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.4382, Train Error: 0.7792\n",
      "  Val Loss: 0.6660, Val Error: 0.3400\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3914, Train Error: 0.7858\n",
      "  Val Loss: 0.6574, Val Error: 0.3000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3849, Train Error: 0.7767\n",
      "  Val Loss: 0.6659, Val Error: 0.3600\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3666, Train Error: 0.7875\n",
      "  Val Loss: 0.6554, Val Error: 0.2400\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3387, Train Error: 0.7775\n",
      "  Val Loss: 0.6683, Val Error: 0.5000\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.3317, Train Error: 0.7817\n",
      "  Val Loss: 0.6629, Val Error: 0.4200\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.3256, Train Error: 0.7725\n",
      "  Val Loss: 0.6630, Val Error: 0.2800\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.3147, Train Error: 0.7592\n",
      "  Val Loss: 0.6651, Val Error: 0.3600\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.3062, Train Error: 0.7708\n",
      "  Val Loss: 0.6632, Val Error: 0.4000\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2996, Train Error: 0.7758\n",
      "  Val Loss: 0.6659, Val Error: 0.3400\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2907, Train Error: 0.7550\n",
      "  Val Loss: 0.6648, Val Error: 0.3200\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2921, Train Error: 0.7708\n",
      "  Val Loss: 0.6640, Val Error: 0.3400\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2839, Train Error: 0.7550\n",
      "  Val Loss: 0.6658, Val Error: 0.3200\n",
      "Early stopping triggered after epoch 23\n",
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=0.5, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.3754, Train Error: 0.8342\n",
      "  Val Loss: 0.7105, Val Error: 0.5000\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8652, Train Error: 0.8050\n",
      "  Val Loss: 0.6867, Val Error: 0.4600\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.7028, Train Error: 0.8292\n",
      "  Val Loss: 0.7022, Val Error: 0.4800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.6077, Train Error: 0.8133\n",
      "  Val Loss: 0.6941, Val Error: 0.4800\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5681, Train Error: 0.7983\n",
      "  Val Loss: 0.6938, Val Error: 0.4200\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5301, Train Error: 0.8000\n",
      "  Val Loss: 0.6956, Val Error: 0.4400\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4902, Train Error: 0.7967\n",
      "  Val Loss: 0.6842, Val Error: 0.4400\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4625, Train Error: 0.7942\n",
      "  Val Loss: 0.6717, Val Error: 0.3400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4289, Train Error: 0.7833\n",
      "  Val Loss: 0.6683, Val Error: 0.4600\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.4136, Train Error: 0.7833\n",
      "  Val Loss: 0.6603, Val Error: 0.3800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3848, Train Error: 0.7825\n",
      "  Val Loss: 0.6789, Val Error: 0.4400\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3762, Train Error: 0.7800\n",
      "  Val Loss: 0.6700, Val Error: 0.4400\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3613, Train Error: 0.7733\n",
      "  Val Loss: 0.6820, Val Error: 0.4600\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3491, Train Error: 0.7767\n",
      "  Val Loss: 0.6786, Val Error: 0.4400\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.3371, Train Error: 0.7658\n",
      "  Val Loss: 0.6777, Val Error: 0.4000\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.3328, Train Error: 0.7667\n",
      "  Val Loss: 0.6811, Val Error: 0.4200\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.3191, Train Error: 0.7558\n",
      "  Val Loss: 0.6754, Val Error: 0.4400\n",
      "Early stopping triggered after epoch 18\n",
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=1.0, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.9778, Train Error: 0.8150\n",
      "  Val Loss: 0.6686, Val Error: 0.4200\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8108, Train Error: 0.8317\n",
      "  Val Loss: 0.6727, Val Error: 0.4400\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.6912, Train Error: 0.8225\n",
      "  Val Loss: 0.6743, Val Error: 0.4400\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.5828, Train Error: 0.8142\n",
      "  Val Loss: 0.6693, Val Error: 0.4000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5634, Train Error: 0.8025\n",
      "  Val Loss: 0.6672, Val Error: 0.4200\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5150, Train Error: 0.8258\n",
      "  Val Loss: 0.6587, Val Error: 0.3400\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4770, Train Error: 0.7967\n",
      "  Val Loss: 0.6712, Val Error: 0.4000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4422, Train Error: 0.8008\n",
      "  Val Loss: 0.6747, Val Error: 0.4400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4084, Train Error: 0.7858\n",
      "  Val Loss: 0.6671, Val Error: 0.3400\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3984, Train Error: 0.7892\n",
      "  Val Loss: 0.6662, Val Error: 0.3600\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3880, Train Error: 0.7917\n",
      "  Val Loss: 0.6698, Val Error: 0.4000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3712, Train Error: 0.7800\n",
      "  Val Loss: 0.6677, Val Error: 0.3400\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3460, Train Error: 0.7717\n",
      "  Val Loss: 0.6644, Val Error: 0.3400\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3493, Train Error: 0.7717\n",
      "  Val Loss: 0.6698, Val Error: 0.3400\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.3296, Train Error: 0.7700\n",
      "  Val Loss: 0.6677, Val Error: 0.3800\n",
      "Early stopping triggered after epoch 16\n",
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=1.0, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.0238, Train Error: 0.8083\n",
      "  Val Loss: 0.6697, Val Error: 0.3800\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8045, Train Error: 0.8233\n",
      "  Val Loss: 0.6672, Val Error: 0.3600\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.6319, Train Error: 0.8100\n",
      "  Val Loss: 0.6695, Val Error: 0.4400\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.6002, Train Error: 0.8158\n",
      "  Val Loss: 0.6590, Val Error: 0.3600\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5403, Train Error: 0.8167\n",
      "  Val Loss: 0.6860, Val Error: 0.4000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5031, Train Error: 0.8117\n",
      "  Val Loss: 0.6613, Val Error: 0.3200\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4576, Train Error: 0.8233\n",
      "  Val Loss: 0.6653, Val Error: 0.3200\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4304, Train Error: 0.8033\n",
      "  Val Loss: 0.6672, Val Error: 0.4400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4344, Train Error: 0.7983\n",
      "  Val Loss: 0.6740, Val Error: 0.4200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3961, Train Error: 0.7942\n",
      "  Val Loss: 0.6759, Val Error: 0.3800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3931, Train Error: 0.8000\n",
      "  Val Loss: 0.6807, Val Error: 0.4200\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3618, Train Error: 0.8042\n",
      "  Val Loss: 0.6825, Val Error: 0.4200\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3530, Train Error: 0.7733\n",
      "  Val Loss: 0.6727, Val Error: 0.3800\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3321, Train Error: 0.7900\n",
      "  Val Loss: 0.6752, Val Error: 0.3400\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.3270, Train Error: 0.7967\n",
      "  Val Loss: 0.6678, Val Error: 0.3800\n",
      "Early stopping triggered after epoch 16\n",
      "Testing configuration: lr=0.0001, lambda_u=10, temperature=1.0, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.2708, Train Error: 0.8333\n",
      "  Val Loss: 0.6930, Val Error: 0.4400\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.8220, Train Error: 0.8192\n",
      "  Val Loss: 0.6976, Val Error: 0.5600\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.6967, Train Error: 0.8142\n",
      "  Val Loss: 0.6833, Val Error: 0.5000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.6081, Train Error: 0.8058\n",
      "  Val Loss: 0.6730, Val Error: 0.3200\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5450, Train Error: 0.8042\n",
      "  Val Loss: 0.6861, Val Error: 0.5000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5018, Train Error: 0.7950\n",
      "  Val Loss: 0.6713, Val Error: 0.5000\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4675, Train Error: 0.8025\n",
      "  Val Loss: 0.6771, Val Error: 0.4600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4490, Train Error: 0.7967\n",
      "  Val Loss: 0.6677, Val Error: 0.3800\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.4315, Train Error: 0.8017\n",
      "  Val Loss: 0.6717, Val Error: 0.3200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.4110, Train Error: 0.7850\n",
      "  Val Loss: 0.6656, Val Error: 0.4200\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3982, Train Error: 0.7917\n",
      "  Val Loss: 0.6638, Val Error: 0.3400\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3533, Train Error: 0.7783\n",
      "  Val Loss: 0.6701, Val Error: 0.3400\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3635, Train Error: 0.7842\n",
      "  Val Loss: 0.6662, Val Error: 0.3400\n",
      "Early stopping triggered after epoch 14\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=0.5, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 2.5100, Train Error: 0.8142\n",
      "  Val Loss: 0.6898, Val Error: 0.3800\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.6903, Train Error: 0.8317\n",
      "  Val Loss: 0.6682, Val Error: 0.3400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.1479, Train Error: 0.8267\n",
      "  Val Loss: 0.7091, Val Error: 0.4600\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.9040, Train Error: 0.8317\n",
      "  Val Loss: 0.7110, Val Error: 0.5400\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.7607, Train Error: 0.8200\n",
      "  Val Loss: 0.7006, Val Error: 0.4800\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5796, Train Error: 0.8125\n",
      "  Val Loss: 0.6879, Val Error: 0.4600\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4961, Train Error: 0.8150\n",
      "  Val Loss: 0.6926, Val Error: 0.5200\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4583, Train Error: 0.8208\n",
      "  Val Loss: 0.6967, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3960, Train Error: 0.8075\n",
      "  Val Loss: 0.6919, Val Error: 0.5000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3616, Train Error: 0.7967\n",
      "  Val Loss: 0.6902, Val Error: 0.4600\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3326, Train Error: 0.7983\n",
      "  Val Loss: 0.6908, Val Error: 0.4600\n",
      "Early stopping triggered after epoch 12\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=0.5, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 2.3042, Train Error: 0.8383\n",
      "  Val Loss: 0.6994, Val Error: 0.4600\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.6684, Train Error: 0.8283\n",
      "  Val Loss: 0.6955, Val Error: 0.5200\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.1418, Train Error: 0.8058\n",
      "  Val Loss: 0.6926, Val Error: 0.4800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8690, Train Error: 0.7958\n",
      "  Val Loss: 0.6999, Val Error: 0.5000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6826, Train Error: 0.8108\n",
      "  Val Loss: 0.6865, Val Error: 0.5000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5618, Train Error: 0.8142\n",
      "  Val Loss: 0.6869, Val Error: 0.4800\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4848, Train Error: 0.8075\n",
      "  Val Loss: 0.6846, Val Error: 0.4800\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4257, Train Error: 0.8025\n",
      "  Val Loss: 0.6831, Val Error: 0.4200\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3821, Train Error: 0.8050\n",
      "  Val Loss: 0.6808, Val Error: 0.3600\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3409, Train Error: 0.7992\n",
      "  Val Loss: 0.6835, Val Error: 0.3800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3259, Train Error: 0.7758\n",
      "  Val Loss: 0.6837, Val Error: 0.4200\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3044, Train Error: 0.7967\n",
      "  Val Loss: 0.6881, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3046, Train Error: 0.8275\n",
      "  Val Loss: 0.6895, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2902, Train Error: 0.8258\n",
      "  Val Loss: 0.6829, Val Error: 0.4000\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2796, Train Error: 0.7825\n",
      "  Val Loss: 0.6838, Val Error: 0.3800\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2760, Train Error: 0.7867\n",
      "  Val Loss: 0.6856, Val Error: 0.4800\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2666, Train Error: 0.7692\n",
      "  Val Loss: 0.6883, Val Error: 0.4800\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2664, Train Error: 0.7717\n",
      "  Val Loss: 0.6865, Val Error: 0.4000\n",
      "Early stopping triggered after epoch 19\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=0.5, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 3.1634, Train Error: 0.8342\n",
      "  Val Loss: 0.7633, Val Error: 0.5000\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.8990, Train Error: 0.8200\n",
      "  Val Loss: 0.6940, Val Error: 0.5400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.2659, Train Error: 0.8375\n",
      "  Val Loss: 0.6908, Val Error: 0.5200\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8677, Train Error: 0.8075\n",
      "  Val Loss: 0.6776, Val Error: 0.4200\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.7166, Train Error: 0.8200\n",
      "  Val Loss: 0.6777, Val Error: 0.4200\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5841, Train Error: 0.8100\n",
      "  Val Loss: 0.6840, Val Error: 0.4600\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4908, Train Error: 0.8250\n",
      "  Val Loss: 0.6864, Val Error: 0.4200\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4505, Train Error: 0.8058\n",
      "  Val Loss: 0.6803, Val Error: 0.4400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3842, Train Error: 0.8033\n",
      "  Val Loss: 0.6821, Val Error: 0.3800\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3604, Train Error: 0.8258\n",
      "  Val Loss: 0.6960, Val Error: 0.5000\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3464, Train Error: 0.8208\n",
      "  Val Loss: 0.6802, Val Error: 0.4000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3181, Train Error: 0.7933\n",
      "  Val Loss: 0.6799, Val Error: 0.3600\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3026, Train Error: 0.7933\n",
      "  Val Loss: 0.6841, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2927, Train Error: 0.8008\n",
      "  Val Loss: 0.6858, Val Error: 0.4200\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2820, Train Error: 0.7867\n",
      "  Val Loss: 0.6858, Val Error: 0.4400\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2814, Train Error: 0.7758\n",
      "  Val Loss: 0.6845, Val Error: 0.4200\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2733, Train Error: 0.7792\n",
      "  Val Loss: 0.6864, Val Error: 0.4000\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2726, Train Error: 0.8050\n",
      "  Val Loss: 0.6861, Val Error: 0.4600\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2673, Train Error: 0.8067\n",
      "  Val Loss: 0.6856, Val Error: 0.4000\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2633, Train Error: 0.8192\n",
      "  Val Loss: 0.6867, Val Error: 0.5000\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2637, Train Error: 0.8275\n",
      "  Val Loss: 0.6886, Val Error: 0.5000\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2589, Train Error: 0.8025\n",
      "  Val Loss: 0.6843, Val Error: 0.3400\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2574, Train Error: 0.7550\n",
      "  Val Loss: 0.6873, Val Error: 0.3600\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.2579, Train Error: 0.7500\n",
      "  Val Loss: 0.6882, Val Error: 0.3800\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2580, Train Error: 0.7558\n",
      "  Val Loss: 0.6873, Val Error: 0.3600\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.2593, Train Error: 0.8033\n",
      "  Val Loss: 0.6886, Val Error: 0.5000\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.2541, Train Error: 0.8258\n",
      "  Val Loss: 0.6871, Val Error: 0.4800\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.2556, Train Error: 0.8108\n",
      "  Val Loss: 0.6862, Val Error: 0.4000\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.2528, Train Error: 0.8133\n",
      "  Val Loss: 0.6876, Val Error: 0.4600\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.2553, Train Error: 0.8208\n",
      "  Val Loss: 0.6881, Val Error: 0.4800\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.2548, Train Error: 0.7783\n",
      "  Val Loss: 0.6871, Val Error: 0.3600\n",
      "Early stopping triggered after epoch 32\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=1.0, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 3.8286, Train Error: 0.8350\n",
      "  Val Loss: 0.7243, Val Error: 0.6000\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.7225, Train Error: 0.8325\n",
      "  Val Loss: 0.7054, Val Error: 0.4800\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.0867, Train Error: 0.8333\n",
      "  Val Loss: 0.7019, Val Error: 0.5400\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.7940, Train Error: 0.8233\n",
      "  Val Loss: 0.6876, Val Error: 0.4400\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6001, Train Error: 0.8142\n",
      "  Val Loss: 0.6906, Val Error: 0.4600\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5130, Train Error: 0.8200\n",
      "  Val Loss: 0.6908, Val Error: 0.4800\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4417, Train Error: 0.8167\n",
      "  Val Loss: 0.6868, Val Error: 0.4200\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3914, Train Error: 0.7933\n",
      "  Val Loss: 0.6837, Val Error: 0.4200\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3577, Train Error: 0.7800\n",
      "  Val Loss: 0.6862, Val Error: 0.4200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3369, Train Error: 0.7925\n",
      "  Val Loss: 0.6867, Val Error: 0.3600\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3131, Train Error: 0.8083\n",
      "  Val Loss: 0.6874, Val Error: 0.3400\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.2967, Train Error: 0.7992\n",
      "  Val Loss: 0.6872, Val Error: 0.3600\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2882, Train Error: 0.7775\n",
      "  Val Loss: 0.6880, Val Error: 0.4200\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2902, Train Error: 0.7883\n",
      "  Val Loss: 0.6901, Val Error: 0.4400\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2778, Train Error: 0.7875\n",
      "  Val Loss: 0.6930, Val Error: 0.5200\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2750, Train Error: 0.7892\n",
      "  Val Loss: 0.6878, Val Error: 0.3600\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2704, Train Error: 0.7617\n",
      "  Val Loss: 0.6910, Val Error: 0.4800\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2650, Train Error: 0.7558\n",
      "  Val Loss: 0.6920, Val Error: 0.5200\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2618, Train Error: 0.7658\n",
      "  Val Loss: 0.6899, Val Error: 0.3600\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2625, Train Error: 0.7725\n",
      "  Val Loss: 0.6895, Val Error: 0.3400\n",
      "Early stopping triggered after epoch 21\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=1.0, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 2.2258, Train Error: 0.8358\n",
      "  Val Loss: 0.7005, Val Error: 0.5200\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.4151, Train Error: 0.8067\n",
      "  Val Loss: 0.7100, Val Error: 0.5400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.0305, Train Error: 0.8275\n",
      "  Val Loss: 0.6841, Val Error: 0.4400\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8288, Train Error: 0.8192\n",
      "  Val Loss: 0.6991, Val Error: 0.4800\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6290, Train Error: 0.8317\n",
      "  Val Loss: 0.7017, Val Error: 0.5200\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5273, Train Error: 0.8075\n",
      "  Val Loss: 0.7048, Val Error: 0.5400\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4632, Train Error: 0.8117\n",
      "  Val Loss: 0.6972, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3891, Train Error: 0.7917\n",
      "  Val Loss: 0.6907, Val Error: 0.4800\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3620, Train Error: 0.7992\n",
      "  Val Loss: 0.6915, Val Error: 0.4800\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3312, Train Error: 0.7875\n",
      "  Val Loss: 0.6926, Val Error: 0.5400\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3115, Train Error: 0.8075\n",
      "  Val Loss: 0.6989, Val Error: 0.5800\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3026, Train Error: 0.7992\n",
      "  Val Loss: 0.6924, Val Error: 0.5000\n",
      "Early stopping triggered after epoch 13\n",
      "Testing configuration: lr=0.0001, lambda_u=30, temperature=1.0, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 3.3746, Train Error: 0.8208\n",
      "  Val Loss: 0.6904, Val Error: 0.4400\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.8181, Train Error: 0.8442\n",
      "  Val Loss: 0.6619, Val Error: 0.3800\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.1381, Train Error: 0.8367\n",
      "  Val Loss: 0.6634, Val Error: 0.4000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8147, Train Error: 0.8392\n",
      "  Val Loss: 0.6621, Val Error: 0.3000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6268, Train Error: 0.8158\n",
      "  Val Loss: 0.6574, Val Error: 0.2400\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5116, Train Error: 0.8267\n",
      "  Val Loss: 0.6729, Val Error: 0.3000\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4319, Train Error: 0.8100\n",
      "  Val Loss: 0.6772, Val Error: 0.3600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.4026, Train Error: 0.8242\n",
      "  Val Loss: 0.6835, Val Error: 0.4200\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3705, Train Error: 0.8133\n",
      "  Val Loss: 0.6856, Val Error: 0.4600\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3370, Train Error: 0.8042\n",
      "  Val Loss: 0.6794, Val Error: 0.3400\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3101, Train Error: 0.7858\n",
      "  Val Loss: 0.6835, Val Error: 0.4400\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3007, Train Error: 0.7883\n",
      "  Val Loss: 0.6813, Val Error: 0.3600\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2913, Train Error: 0.7883\n",
      "  Val Loss: 0.6855, Val Error: 0.4400\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2849, Train Error: 0.7808\n",
      "  Val Loss: 0.6853, Val Error: 0.3400\n",
      "Early stopping triggered after epoch 15\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=0.5, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 4.4123, Train Error: 0.8367\n",
      "  Val Loss: 0.7123, Val Error: 0.4600\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.2755, Train Error: 0.8308\n",
      "  Val Loss: 0.7148, Val Error: 0.5600\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.3552, Train Error: 0.8108\n",
      "  Val Loss: 0.6938, Val Error: 0.5000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8908, Train Error: 0.8383\n",
      "  Val Loss: 0.7055, Val Error: 0.5800\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6482, Train Error: 0.8133\n",
      "  Val Loss: 0.7009, Val Error: 0.4400\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5099, Train Error: 0.8233\n",
      "  Val Loss: 0.6976, Val Error: 0.4200\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4081, Train Error: 0.8225\n",
      "  Val Loss: 0.7027, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3697, Train Error: 0.8317\n",
      "  Val Loss: 0.7019, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3407, Train Error: 0.8250\n",
      "  Val Loss: 0.6978, Val Error: 0.4400\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3274, Train Error: 0.8117\n",
      "  Val Loss: 0.6901, Val Error: 0.4800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3061, Train Error: 0.8208\n",
      "  Val Loss: 0.6901, Val Error: 0.5000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3119, Train Error: 0.7967\n",
      "  Val Loss: 0.6923, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2951, Train Error: 0.8267\n",
      "  Val Loss: 0.6962, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2906, Train Error: 0.8383\n",
      "  Val Loss: 0.6940, Val Error: 0.5000\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2920, Train Error: 0.8325\n",
      "  Val Loss: 0.6910, Val Error: 0.4800\n",
      "Early stopping triggered after epoch 16\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=0.5, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 5.6344, Train Error: 0.8417\n",
      "  Val Loss: 0.7210, Val Error: 0.5400\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.7078, Train Error: 0.8317\n",
      "  Val Loss: 0.6831, Val Error: 0.4200\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.5482, Train Error: 0.8192\n",
      "  Val Loss: 0.7021, Val Error: 0.4200\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.9672, Train Error: 0.8075\n",
      "  Val Loss: 0.7212, Val Error: 0.5000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6857, Train Error: 0.8250\n",
      "  Val Loss: 0.6953, Val Error: 0.4800\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5272, Train Error: 0.8267\n",
      "  Val Loss: 0.7275, Val Error: 0.5000\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4351, Train Error: 0.8333\n",
      "  Val Loss: 0.6922, Val Error: 0.5600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3864, Train Error: 0.8133\n",
      "  Val Loss: 0.6921, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3368, Train Error: 0.8183\n",
      "  Val Loss: 0.6936, Val Error: 0.4000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3358, Train Error: 0.8225\n",
      "  Val Loss: 0.7049, Val Error: 0.5000\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3195, Train Error: 0.8383\n",
      "  Val Loss: 0.6967, Val Error: 0.5000\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.2973, Train Error: 0.8133\n",
      "  Val Loss: 0.6925, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2963, Train Error: 0.8283\n",
      "  Val Loss: 0.6893, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2862, Train Error: 0.8158\n",
      "  Val Loss: 0.6882, Val Error: 0.3800\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2852, Train Error: 0.8267\n",
      "  Val Loss: 0.6984, Val Error: 0.5000\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2837, Train Error: 0.8383\n",
      "  Val Loss: 0.6968, Val Error: 0.5000\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2816, Train Error: 0.8242\n",
      "  Val Loss: 0.6907, Val Error: 0.4000\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2734, Train Error: 0.8225\n",
      "  Val Loss: 0.6899, Val Error: 0.5000\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2715, Train Error: 0.8283\n",
      "  Val Loss: 0.6902, Val Error: 0.5000\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2665, Train Error: 0.8283\n",
      "  Val Loss: 0.6911, Val Error: 0.5000\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2687, Train Error: 0.8033\n",
      "  Val Loss: 0.6901, Val Error: 0.4200\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2683, Train Error: 0.8017\n",
      "  Val Loss: 0.6919, Val Error: 0.5000\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2692, Train Error: 0.8117\n",
      "  Val Loss: 0.6903, Val Error: 0.4200\n",
      "Early stopping triggered after epoch 24\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=0.5, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 4.7323, Train Error: 0.8375\n",
      "  Val Loss: 0.7285, Val Error: 0.5600\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.7417, Train Error: 0.8150\n",
      "  Val Loss: 0.6937, Val Error: 0.5000\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.4088, Train Error: 0.8167\n",
      "  Val Loss: 0.6850, Val Error: 0.4000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8839, Train Error: 0.8275\n",
      "  Val Loss: 0.6769, Val Error: 0.3600\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6140, Train Error: 0.8158\n",
      "  Val Loss: 0.6789, Val Error: 0.3400\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4700, Train Error: 0.8267\n",
      "  Val Loss: 0.6833, Val Error: 0.4600\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4061, Train Error: 0.8133\n",
      "  Val Loss: 0.6888, Val Error: 0.4600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3632, Train Error: 0.8083\n",
      "  Val Loss: 0.6922, Val Error: 0.5200\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3301, Train Error: 0.8242\n",
      "  Val Loss: 0.6950, Val Error: 0.5000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3076, Train Error: 0.8225\n",
      "  Val Loss: 0.6895, Val Error: 0.5000\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.2966, Train Error: 0.7833\n",
      "  Val Loss: 0.6900, Val Error: 0.3600\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.2920, Train Error: 0.7958\n",
      "  Val Loss: 0.6915, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2828, Train Error: 0.8375\n",
      "  Val Loss: 0.6938, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2802, Train Error: 0.8367\n",
      "  Val Loss: 0.6922, Val Error: 0.4800\n",
      "Early stopping triggered after epoch 15\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=1.0, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 3.2962, Train Error: 0.8483\n",
      "  Val Loss: 0.6897, Val Error: 0.5000\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.0432, Train Error: 0.8325\n",
      "  Val Loss: 0.6815, Val Error: 0.5400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.2661, Train Error: 0.8325\n",
      "  Val Loss: 0.6745, Val Error: 0.4000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.7503, Train Error: 0.8317\n",
      "  Val Loss: 0.6907, Val Error: 0.5200\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5671, Train Error: 0.8242\n",
      "  Val Loss: 0.6871, Val Error: 0.4400\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4479, Train Error: 0.8092\n",
      "  Val Loss: 0.6874, Val Error: 0.4600\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.3854, Train Error: 0.8125\n",
      "  Val Loss: 0.6883, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3493, Train Error: 0.8208\n",
      "  Val Loss: 0.6918, Val Error: 0.4800\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3152, Train Error: 0.8075\n",
      "  Val Loss: 0.6913, Val Error: 0.5200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3092, Train Error: 0.7983\n",
      "  Val Loss: 0.6923, Val Error: 0.4800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.2997, Train Error: 0.8208\n",
      "  Val Loss: 0.6910, Val Error: 0.4200\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.2811, Train Error: 0.7767\n",
      "  Val Loss: 0.6906, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2791, Train Error: 0.7808\n",
      "  Val Loss: 0.6899, Val Error: 0.3800\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2802, Train Error: 0.7833\n",
      "  Val Loss: 0.6883, Val Error: 0.3600\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2788, Train Error: 0.7858\n",
      "  Val Loss: 0.6906, Val Error: 0.4800\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2736, Train Error: 0.7642\n",
      "  Val Loss: 0.6910, Val Error: 0.4000\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2760, Train Error: 0.7633\n",
      "  Val Loss: 0.6905, Val Error: 0.3800\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2722, Train Error: 0.7683\n",
      "  Val Loss: 0.6899, Val Error: 0.3200\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2682, Train Error: 0.7608\n",
      "  Val Loss: 0.6897, Val Error: 0.3200\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2719, Train Error: 0.7658\n",
      "  Val Loss: 0.6902, Val Error: 0.3800\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2700, Train Error: 0.7708\n",
      "  Val Loss: 0.6896, Val Error: 0.3800\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2664, Train Error: 0.7808\n",
      "  Val Loss: 0.6904, Val Error: 0.4600\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2649, Train Error: 0.7833\n",
      "  Val Loss: 0.6906, Val Error: 0.4000\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.2627, Train Error: 0.7625\n",
      "  Val Loss: 0.6893, Val Error: 0.3800\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2664, Train Error: 0.7658\n",
      "  Val Loss: 0.6897, Val Error: 0.4200\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.2647, Train Error: 0.7642\n",
      "  Val Loss: 0.6909, Val Error: 0.4200\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.2614, Train Error: 0.7625\n",
      "  Val Loss: 0.6905, Val Error: 0.4000\n",
      "Early stopping triggered after epoch 28\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=1.0, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 4.4229, Train Error: 0.8167\n",
      "  Val Loss: 0.7143, Val Error: 0.5200\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.2620, Train Error: 0.8267\n",
      "  Val Loss: 0.7145, Val Error: 0.5800\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.3417, Train Error: 0.8325\n",
      "  Val Loss: 0.6912, Val Error: 0.4800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8173, Train Error: 0.8117\n",
      "  Val Loss: 0.7007, Val Error: 0.5400\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5785, Train Error: 0.8208\n",
      "  Val Loss: 0.7021, Val Error: 0.4600\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4833, Train Error: 0.8250\n",
      "  Val Loss: 0.7036, Val Error: 0.6200\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.3910, Train Error: 0.8100\n",
      "  Val Loss: 0.6951, Val Error: 0.5400\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3540, Train Error: 0.8058\n",
      "  Val Loss: 0.6934, Val Error: 0.4400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3291, Train Error: 0.8092\n",
      "  Val Loss: 0.6928, Val Error: 0.4200\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3165, Train Error: 0.7992\n",
      "  Val Loss: 0.6932, Val Error: 0.4400\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3068, Train Error: 0.7658\n",
      "  Val Loss: 0.6933, Val Error: 0.5200\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3012, Train Error: 0.7783\n",
      "  Val Loss: 0.6917, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2893, Train Error: 0.7942\n",
      "  Val Loss: 0.6895, Val Error: 0.4000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2930, Train Error: 0.7725\n",
      "  Val Loss: 0.6900, Val Error: 0.4200\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2889, Train Error: 0.7808\n",
      "  Val Loss: 0.6900, Val Error: 0.4400\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2831, Train Error: 0.7650\n",
      "  Val Loss: 0.6901, Val Error: 0.4000\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2795, Train Error: 0.7800\n",
      "  Val Loss: 0.6898, Val Error: 0.3200\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2774, Train Error: 0.7642\n",
      "  Val Loss: 0.6905, Val Error: 0.3600\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2770, Train Error: 0.7608\n",
      "  Val Loss: 0.6911, Val Error: 0.4200\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2801, Train Error: 0.7675\n",
      "  Val Loss: 0.6903, Val Error: 0.4000\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2730, Train Error: 0.7783\n",
      "  Val Loss: 0.6904, Val Error: 0.3800\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2743, Train Error: 0.7825\n",
      "  Val Loss: 0.6901, Val Error: 0.4000\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2728, Train Error: 0.7583\n",
      "  Val Loss: 0.6906, Val Error: 0.4200\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.2709, Train Error: 0.7725\n",
      "  Val Loss: 0.6902, Val Error: 0.4800\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2663, Train Error: 0.7642\n",
      "  Val Loss: 0.6897, Val Error: 0.3400\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.2745, Train Error: 0.7625\n",
      "  Val Loss: 0.6895, Val Error: 0.3600\n",
      "Early stopping triggered after epoch 27\n",
      "Testing configuration: lr=0.0001, lambda_u=50, temperature=1.0, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 5.7951, Train Error: 0.8442\n",
      "  Val Loss: 0.7290, Val Error: 0.4800\n",
      "Epoch 2/50\n",
      "  Train Loss: 2.6273, Train Error: 0.8325\n",
      "  Val Loss: 0.6957, Val Error: 0.4400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.3386, Train Error: 0.8242\n",
      "  Val Loss: 0.6809, Val Error: 0.5000\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8258, Train Error: 0.8158\n",
      "  Val Loss: 0.6848, Val Error: 0.4000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5458, Train Error: 0.8292\n",
      "  Val Loss: 0.6845, Val Error: 0.3800\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4412, Train Error: 0.8333\n",
      "  Val Loss: 0.6847, Val Error: 0.4200\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.3773, Train Error: 0.8242\n",
      "  Val Loss: 0.6877, Val Error: 0.3800\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3373, Train Error: 0.8175\n",
      "  Val Loss: 0.6900, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3133, Train Error: 0.8017\n",
      "  Val Loss: 0.6883, Val Error: 0.4800\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3061, Train Error: 0.7875\n",
      "  Val Loss: 0.6898, Val Error: 0.4800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3020, Train Error: 0.8050\n",
      "  Val Loss: 0.6907, Val Error: 0.5600\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.2831, Train Error: 0.7700\n",
      "  Val Loss: 0.6897, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.2791, Train Error: 0.7892\n",
      "  Val Loss: 0.6896, Val Error: 0.4000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.2763, Train Error: 0.7742\n",
      "  Val Loss: 0.6889, Val Error: 0.3600\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2757, Train Error: 0.7867\n",
      "  Val Loss: 0.6899, Val Error: 0.4200\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2772, Train Error: 0.7783\n",
      "  Val Loss: 0.6893, Val Error: 0.3400\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2745, Train Error: 0.7733\n",
      "  Val Loss: 0.6893, Val Error: 0.3600\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2721, Train Error: 0.7642\n",
      "  Val Loss: 0.6896, Val Error: 0.3200\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2686, Train Error: 0.7700\n",
      "  Val Loss: 0.6901, Val Error: 0.3600\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2663, Train Error: 0.7650\n",
      "  Val Loss: 0.6893, Val Error: 0.3800\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2663, Train Error: 0.7592\n",
      "  Val Loss: 0.6894, Val Error: 0.4000\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2662, Train Error: 0.7650\n",
      "  Val Loss: 0.6890, Val Error: 0.3800\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2645, Train Error: 0.7683\n",
      "  Val Loss: 0.6885, Val Error: 0.3200\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.2648, Train Error: 0.7808\n",
      "  Val Loss: 0.6880, Val Error: 0.3800\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2598, Train Error: 0.7508\n",
      "  Val Loss: 0.6888, Val Error: 0.3600\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.2627, Train Error: 0.7667\n",
      "  Val Loss: 0.6880, Val Error: 0.3400\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.2595, Train Error: 0.7633\n",
      "  Val Loss: 0.6894, Val Error: 0.3600\n",
      "Early stopping triggered after epoch 28\n",
      "Testing configuration: lr=0.0001, lambda_u=80, temperature=0.5, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 5.7952, Train Error: 0.8442\n",
      "  Val Loss: 0.7500, Val Error: 0.5400\n",
      "Epoch 2/50\n",
      "  Train Loss: 3.0563, Train Error: 0.8342\n",
      "  Val Loss: 0.6938, Val Error: 0.4400\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.6231, Train Error: 0.8400\n",
      "  Val Loss: 0.6938, Val Error: 0.4800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.9804, Train Error: 0.8358\n",
      "  Val Loss: 0.7043, Val Error: 0.5000\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6033, Train Error: 0.8342\n",
      "  Val Loss: 0.7003, Val Error: 0.6000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4838, Train Error: 0.8275\n",
      "  Val Loss: 0.6909, Val Error: 0.5400\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4149, Train Error: 0.8192\n",
      "  Val Loss: 0.6881, Val Error: 0.3600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3618, Train Error: 0.8150\n",
      "  Val Loss: 0.6905, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3446, Train Error: 0.8375\n",
      "  Val Loss: 0.6962, Val Error: 0.5000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3231, Train Error: 0.8383\n",
      "  Val Loss: 0.6992, Val Error: 0.5000\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3112, Train Error: 0.8308\n",
      "  Val Loss: 0.6930, Val Error: 0.4800\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3161, Train Error: 0.8108\n",
      "  Val Loss: 0.6916, Val Error: 0.5000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3043, Train Error: 0.8283\n",
      "  Val Loss: 0.6928, Val Error: 0.5000\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3027, Train Error: 0.8283\n",
      "  Val Loss: 0.6894, Val Error: 0.4200\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2961, Train Error: 0.7908\n",
      "  Val Loss: 0.6893, Val Error: 0.4600\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2973, Train Error: 0.8317\n",
      "  Val Loss: 0.6934, Val Error: 0.5200\n",
      "Early stopping triggered after epoch 17\n",
      "Testing configuration: lr=0.0001, lambda_u=80, temperature=0.5, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 9.0233, Train Error: 0.8317\n",
      "  Val Loss: 0.6877, Val Error: 0.4200\n",
      "Epoch 2/50\n",
      "  Train Loss: 3.0301, Train Error: 0.8317\n",
      "  Val Loss: 0.7117, Val Error: 0.5200\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.6232, Train Error: 0.8150\n",
      "  Val Loss: 0.6767, Val Error: 0.4800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.9975, Train Error: 0.8367\n",
      "  Val Loss: 0.6976, Val Error: 0.5600\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6776, Train Error: 0.8400\n",
      "  Val Loss: 0.6885, Val Error: 0.5000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5130, Train Error: 0.8300\n",
      "  Val Loss: 0.7113, Val Error: 0.5000\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4466, Train Error: 0.8483\n",
      "  Val Loss: 0.6981, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3861, Train Error: 0.8275\n",
      "  Val Loss: 0.6891, Val Error: 0.4600\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3352, Train Error: 0.8308\n",
      "  Val Loss: 0.7028, Val Error: 0.5000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3456, Train Error: 0.8383\n",
      "  Val Loss: 0.7017, Val Error: 0.5000\n",
      "Early stopping triggered after epoch 11\n",
      "Testing configuration: lr=0.0001, lambda_u=80, temperature=0.5, mixup_alpha=1.0\n",
      "Epoch 1/50\n",
      "  Train Loss: 9.0243, Train Error: 0.8433\n",
      "  Val Loss: 0.6822, Val Error: 0.4000\n",
      "Epoch 2/50\n",
      "  Train Loss: 4.0532, Train Error: 0.8317\n",
      "  Val Loss: 0.7170, Val Error: 0.4600\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.9144, Train Error: 0.8183\n",
      "  Val Loss: 0.7029, Val Error: 0.5400\n",
      "Epoch 4/50\n",
      "  Train Loss: 1.0378, Train Error: 0.8408\n",
      "  Val Loss: 0.6910, Val Error: 0.4600\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.7165, Train Error: 0.8233\n",
      "  Val Loss: 0.7165, Val Error: 0.5000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.5509, Train Error: 0.8250\n",
      "  Val Loss: 0.6893, Val Error: 0.4400\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4332, Train Error: 0.8367\n",
      "  Val Loss: 0.6898, Val Error: 0.5000\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3804, Train Error: 0.8492\n",
      "  Val Loss: 0.6878, Val Error: 0.5000\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3479, Train Error: 0.8267\n",
      "  Val Loss: 0.6919, Val Error: 0.5000\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3257, Train Error: 0.8267\n",
      "  Val Loss: 0.6922, Val Error: 0.5400\n",
      "Early stopping triggered after epoch 11\n",
      "Testing configuration: lr=0.0001, lambda_u=80, temperature=1.0, mixup_alpha=0.5\n",
      "Epoch 1/50\n",
      "  Train Loss: 8.4044, Train Error: 0.8283\n",
      "  Val Loss: 0.7089, Val Error: 0.5200\n",
      "Epoch 2/50\n",
      "  Train Loss: 3.4085, Train Error: 0.8117\n",
      "  Val Loss: 0.6880, Val Error: 0.4800\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.5573, Train Error: 0.8267\n",
      "  Val Loss: 0.6673, Val Error: 0.4200\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8165, Train Error: 0.8250\n",
      "  Val Loss: 0.6825, Val Error: 0.4800\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.5587, Train Error: 0.8217\n",
      "  Val Loss: 0.6920, Val Error: 0.3800\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4329, Train Error: 0.8217\n",
      "  Val Loss: 0.6842, Val Error: 0.3800\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.3786, Train Error: 0.8283\n",
      "  Val Loss: 0.6887, Val Error: 0.4600\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3460, Train Error: 0.8258\n",
      "  Val Loss: 0.6907, Val Error: 0.4400\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3287, Train Error: 0.8000\n",
      "  Val Loss: 0.6896, Val Error: 0.3800\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3114, Train Error: 0.8042\n",
      "  Val Loss: 0.6904, Val Error: 0.4400\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3106, Train Error: 0.7908\n",
      "  Val Loss: 0.6925, Val Error: 0.5200\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3063, Train Error: 0.7975\n",
      "  Val Loss: 0.6900, Val Error: 0.4000\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3052, Train Error: 0.7850\n",
      "  Val Loss: 0.6915, Val Error: 0.4200\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3012, Train Error: 0.7725\n",
      "  Val Loss: 0.6913, Val Error: 0.4600\n",
      "Early stopping triggered after epoch 15\n",
      "Testing configuration: lr=0.0001, lambda_u=80, temperature=1.0, mixup_alpha=0.75\n",
      "Epoch 1/50\n",
      "  Train Loss: 7.0560, Train Error: 0.8425\n",
      "  Val Loss: 0.6967, Val Error: 0.5000\n",
      "Epoch 2/50\n",
      "  Train Loss: 3.4414, Train Error: 0.8267\n",
      "  Val Loss: 0.7107, Val Error: 0.5000\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.4712, Train Error: 0.8592\n",
      "  Val Loss: 0.7019, Val Error: 0.5800\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8980, Train Error: 0.8233\n",
      "  Val Loss: 0.6950, Val Error: 0.4800\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.7043, Train Error: 0.8008\n",
      "  Val Loss: 0.6964, Val Error: 0.5000\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.4660, Train Error: 0.8108\n",
      "  Val Loss: 0.6946, Val Error: 0.5600\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.4003, Train Error: 0.8300\n",
      "  Val Loss: 0.6922, Val Error: 0.5400\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.3626, Train Error: 0.8025\n",
      "  Val Loss: 0.6908, Val Error: 0.4800\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.3314, Train Error: 0.8250\n",
      "  Val Loss: 0.6916, Val Error: 0.4600\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.3167, Train Error: 0.7883\n",
      "  Val Loss: 0.6913, Val Error: 0.4800\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.3112, Train Error: 0.7883\n",
      "  Val Loss: 0.6917, Val Error: 0.4400\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.3006, Train Error: 0.7767\n",
      "  Val Loss: 0.6905, Val Error: 0.4400\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.3019, Train Error: 0.7867\n",
      "  Val Loss: 0.6915, Val Error: 0.4600\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.3013, Train Error: 0.7850\n",
      "  Val Loss: 0.6930, Val Error: 0.4000\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.2898, Train Error: 0.7508\n",
      "  Val Loss: 0.6934, Val Error: 0.4400\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.2898, Train Error: 0.7650\n",
      "  Val Loss: 0.6922, Val Error: 0.4200\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.2919, Train Error: 0.7833\n",
      "  Val Loss: 0.6923, Val Error: 0.4000\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.2841, Train Error: 0.7692\n",
      "  Val Loss: 0.6921, Val Error: 0.4400\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.2772, Train Error: 0.7625\n",
      "  Val Loss: 0.6917, Val Error: 0.4400\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.2841, Train Error: 0.7650\n",
      "  Val Loss: 0.6916, Val Error: 0.4600\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.2792, Train Error: 0.7817\n",
      "  Val Loss: 0.6906, Val Error: 0.4200\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.2834, Train Error: 0.7608\n",
      "  Val Loss: 0.6909, Val Error: 0.3800\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.2828, Train Error: 0.7683\n",
      "  Val Loss: 0.6912, Val Error: 0.4800\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.2817, Train Error: 0.7525\n",
      "  Val Loss: 0.6922, Val Error: 0.4800\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2770, Train Error: 0.7700\n",
      "  Val Loss: 0.6905, Val Error: 0.3800\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.2788, Train Error: 0.7533\n",
      "  Val Loss: 0.6919, Val Error: 0.4800\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.2704, Train Error: 0.7592\n",
      "  Val Loss: 0.6922, Val Error: 0.4400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 221\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr, lambda_u, temperature, mixup_alpha \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[0;32m    219\u001b[0m         param_grid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], param_grid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_u\u001b[39m\u001b[38;5;124m\"\u001b[39m], param_grid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m], param_grid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixup_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting configuration: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lambda_u=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_u\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mixup_alpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmixup_alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m     val_err, model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_alpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_err \u001b[38;5;241m<\u001b[39m best_val_error:\n\u001b[0;32m    224\u001b[0m         best_val_error \u001b[38;5;241m=\u001b[39m val_err\n",
      "Cell \u001b[1;32mIn[16], line 148\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(lr, lambda_u, temperature, mixup_alpha, num_epochs)\u001b[0m\n\u001b[0;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Track training metrics\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets_x)  \u001b[38;5;66;03m# Accumulate total loss, scaled by batch size\u001b[39;00m\n\u001b[0;32m    149\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs_x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m inputs_u1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m inputs_u2\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Track total number of samples\u001b[39;00m\n\u001b[0;32m    150\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;28mlen\u001b[39m(targets_x)]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if GRID_SEARCH:\n",
    "    import itertools\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # MixMatch hyperparameters\n",
    "    mixup_alpha = 0.75\n",
    "    temperature = 0.5\n",
    "    lambda_u = 10\n",
    "    num_classes = 2  # Adjust to your dataset\n",
    "\n",
    "    # Data collection for training visualization\n",
    "    best_info = {\n",
    "        'epochs': [],\n",
    "        'tr': {'loss': [], 'err': []},\n",
    "        'va': {'xent': [], 'err': []},\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_error = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    results = []\n",
    "    \n",
    "    # Define sharpening function\n",
    "    def sharpen(probabilities, T):\n",
    "        return torch.pow(probabilities, 1 / T) / torch.sum(torch.pow(probabilities, 1 / T), dim=1, keepdim=True)\n",
    "    \n",
    "    # Define mixup function\n",
    "    def mixup(x1, y1, x2, y2, alpha):\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "        lam = max(lam, 1 - lam)\n",
    "        x_mix = lam * x1 + (1 - lam) * x2\n",
    "        y_mix = lam * y1 + (1 - lam) * y2\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "    class ResNetFeatureExtractor(nn.Module):\n",
    "        def __init__(self, num_classes, dropout_rate=0.3):\n",
    "            super(ResNetFeatureExtractor, self).__init__()\n",
    "            # Use a deeper ResNet architecture\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            \n",
    "            # Remove final layers\n",
    "            self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            \n",
    "            # Add custom classification head\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            \n",
    "            # Two fully connected layers with batch norm\n",
    "            self.fc1 = nn.Linear(2048, 512)\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            features = self.feature_extractor(x)\n",
    "            features = self.avgpool(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            features = self.dropout(features)\n",
    "            features = F.relu(self.bn1(self.fc1(features)))\n",
    "            features = self.dropout(features)\n",
    "            out = self.fc2(features)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    root_path = DATA_DIR\n",
    "    tr_loader, unloader1, unloader2, va_loader, test_loader  = data_utils_pseudo.make_PN_data_loaders_with_unlabeled(\n",
    "        root=root_path,\n",
    "        batch_size=32,\n",
    "        frac_valid=50/850\n",
    "    )\n",
    "    \n",
    "    # Model, optimizer, and criterion\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Define the grid search function\n",
    "    def train_and_evaluate(lr, lambda_u, temperature, mixup_alpha, num_epochs=50):\n",
    "        # Model, optimizer, and criterion\n",
    "        model = ResNetFeatureExtractor(num_classes=num_classes).to(device)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # Training loop\n",
    "        best_val_error = float('inf')\n",
    "        patience_counter = 0\n",
    "        results = []\n",
    "    \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total_samples = 0\n",
    "    \n",
    "            for (inputs_x, targets_x), inputs_u1, inputs_u2 in zip(tr_loader, unloader1, unloader2):\n",
    "                # Transfer to device\n",
    "                inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "                inputs_u1 = inputs_u1[0].to(device)\n",
    "                inputs_u2 = inputs_u2[0].to(device)\n",
    "    \n",
    "                # Forward pass for unlabeled data\n",
    "                with torch.no_grad():\n",
    "                    outputs_u1 = model(inputs_u1)\n",
    "                    outputs_u2 = model(inputs_u2)\n",
    "                    pseudo_labels1 = torch.softmax(outputs_u1, dim=1)\n",
    "                    pseudo_labels2 = torch.softmax(outputs_u2, dim=1)\n",
    "                    p = (pseudo_labels1 + pseudo_labels2) / 2\n",
    "                    pseudo_labels = sharpen(p, temperature)\n",
    "    \n",
    "                # Combine labeled and pseudo-labeled data\n",
    "                all_inputs = torch.cat([inputs_x, inputs_u1, inputs_u2], dim=0)\n",
    "                all_labels = torch.cat([\n",
    "                    torch.nn.functional.one_hot(targets_x, num_classes).float(),\n",
    "                    pseudo_labels, pseudo_labels\n",
    "                ], dim=0)\n",
    "                inputs_mixed, labels_mixed = mixup(all_inputs, all_labels, all_inputs, all_labels, mixup_alpha)\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(inputs_mixed)\n",
    "\n",
    "                supervised_loss = criterion(outputs[:len(targets_x)], targets_x)\n",
    "    \n",
    "                # Compute unsupervised loss (set reduction='sum' and normalize manually for per-sample loss)\n",
    "                unsupervised_loss = F.mse_loss(\n",
    "                    outputs[len(targets_x):], labels_mixed[len(targets_x):])\n",
    "                \n",
    "                # Combine supervised and unsupervised losses\n",
    "                loss = supervised_loss + lambda_u * unsupervised_loss\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track training metrics\n",
    "                total_loss += loss.item() * len(targets_x)  # Accumulate total loss, scaled by batch size\n",
    "                total_samples += inputs_x.size(0) + inputs_u1.size(0) + inputs_u2.size(0)  # Track total number of samples\n",
    "                _, predicted = outputs[:len(targets_x)].max(1)\n",
    "                correct += predicted.eq(targets_x).sum().item()\n",
    "        \n",
    "            # Training metrics\n",
    "            train_loss = total_loss / total_samples\n",
    "            train_err = 1 - correct / total_samples\n",
    "            best_info['tr']['loss'].append(train_loss)\n",
    "            best_info['tr']['err'].append(train_err)\n",
    "        \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs_val, targets_val in va_loader:\n",
    "                    inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "            \n",
    "                    # Forward pass\n",
    "                    outputs_val = model(inputs_val)\n",
    "            \n",
    "                    # Compute validation loss (per-sample average)\n",
    "                    loss_val = criterion(outputs_val, targets_val)  # CrossEntropyLoss defaults to mean\n",
    "                    val_loss += loss_val.item() * inputs_val.size(0)  # Scale by batch size to get total loss for this batch\n",
    "            \n",
    "                    # Compute accuracy\n",
    "                    _, predicted_val = outputs_val.max(1)\n",
    "                    val_correct += predicted_val.eq(targets_val).sum().item()\n",
    "                    val_samples += inputs_val.size(0)\n",
    "            \n",
    "            # Compute per-sample average loss across the entire validation set\n",
    "            val_xent = val_loss / val_samples\n",
    "            val_err = 1 - val_correct / val_samples\n",
    "            val_acc = val_correct / val_samples\n",
    "    \n",
    "            # Early stopping logic\n",
    "            if val_err < best_val_error:\n",
    "                best_val_error = val_err\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "            # Logging\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {total_loss / total_samples:.4f}, Train Error: {1 - correct / total_samples:.4f}\")\n",
    "            print(f\"  Val Loss: {val_xent:.4f}, Val Error: {val_err:.4f}\")\n",
    "    \n",
    "        return val_err, model.state_dict()\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"lr\": [0.0001, 0.00001],\n",
    "        \"lambda_u\": [10, 30, 50, 80],\n",
    "        \"temperature\": [0.5, 1.0],\n",
    "        \"mixup_alpha\": [0.5, 0.75, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    best_model_state = None\n",
    "    best_hyperparams = None\n",
    "    best_val_error = float('inf')\n",
    "    \n",
    "    for lr, lambda_u, temperature, mixup_alpha in itertools.product(\n",
    "            param_grid[\"lr\"], param_grid[\"lambda_u\"], param_grid[\"temperature\"], param_grid[\"mixup_alpha\"]):\n",
    "        print(f\"Testing configuration: lr={lr}, lambda_u={lambda_u}, temperature={temperature}, mixup_alpha={mixup_alpha}\")\n",
    "        val_err, model_state = train_and_evaluate(lr, lambda_u, temperature, mixup_alpha)\n",
    "    \n",
    "        if val_err < best_val_error:\n",
    "            best_val_error = val_err\n",
    "            best_model_state = model_state\n",
    "            best_hyperparams = {\"lr\": lr, \"lambda_u\": lambda_u, \"temperature\": temperature, \"mixup_alpha\": mixup_alpha}\n",
    "    \n",
    "    # Save the best model and hyperparameters\n",
    "    torch.save(best_model_state, \"best_model_grid_search.pth\")\n",
    "    print(f\"Best validation error: {best_val_error}\")\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88051dc4-ad94-401c-98e1-dc6676160c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20242b1c-93ae-46f1-8440-5f4d83e7cf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs152_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
